{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d89016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdfFileObj = open(r\"/home/sample1.pdf\", 'rb')\n",
    "\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "print(len(pdfReader.pages))\n",
    "\n",
    "pageObj = pdfReader.pages[0]\n",
    "\n",
    "print(pageObj.extract_text())\n",
    "\n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44601010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "doc = docx.Document()\n",
    "\n",
    "doc.add_heading('Heading for the document', 0)\n",
    "\n",
    "doc_para = doc.add_paragraph('Your paragraph goes here, ')\n",
    "\n",
    "doc_para.add_run('hey there, bold here').bold = True\n",
    "doc_para.add_run(', and ')\n",
    "doc_para.add_run('these words are italic').italic = True\n",
    "\n",
    "doc.add_page_break()\n",
    "\n",
    "doc.add_heading('Heading level 2', 2)\n",
    "\n",
    "doc.add_picture(r\"/home/index.jpg\")\n",
    "\n",
    "doc.save('new_doc.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b623ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "txt = \"He is a boy. \"\\\n",
    "    \"She is a girl\"\n",
    "\n",
    "word_tokens = word_tokenize(txt)\n",
    "  \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51461c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a42580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(\"Tokenized:\", word_tokens)\n",
    "print(\"Stop Words Removed:\", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(r\"/home/text.txt\")\n",
    " \n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \" + r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6630193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It vijaying meeting better vijayed vijays eats skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"It studies densely is better  meeting studying vijaying vijayed vijays skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "        print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0edf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"Data Science is the best job of the 21st century\"\n",
    "second_sentence = \"Machine learning is the key for data science\"\n",
    "\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69734cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
